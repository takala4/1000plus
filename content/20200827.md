Title: 2020年08月27日
Date: 2020-08-27
Category: Nikki
Tags: 
Slug: 20200827
Authors: takala4
Day: 244



甘利「[情報理論](https://amzn.to/3hzvCJx)」を読んでいる．


開始数ページの情報量とエントロピーの定義の導入の時点で躓いてしまった．


本書ではエントロピー次のように定義している．


> **定義1.2** $n$個の事象がそれぞれ確率$p_{1},...,p_{n}$ で発生するとき，
> どれが発生したかの不確定度を
>
> $$ H(p_{1},...,p_{n}) = - \sum^{n}_{i=1} p_{i} \log p_{i} $$
> 
> と定義し，これをエントロピーと呼ぶ．


不確定度ってなんですか？！となりますよね．
この定義を導入する前に，

> この値（エントロピーの定義にあたる）は，不確定な状況を確定するのに要する平均情報量だと行ってもよい．


というような前置きがあるのもの，すんなり理解できずにいる．


定義を受け入れた上での，
エントロピーの数学的扱いについては様々な訓練を経て理解してきたものの，
そもそもエントロピーって何？なぜそう定義するの？といった
根源的理解が足りていない．



最初の10数ページを何回も読み直しているが，本書とその内容には
それだけ時間をかける価値があると思うので（教科書ではないが）丁寧に読み進めていきたい．