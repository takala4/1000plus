---
documentclass: bxjsarticle
title: 20200219
yesterday: 20200218
tomorrow: 20200220
days: 54
author: Takala
header-includes:
  - \usepackage[margin=1in]{geometry}
  - \usepackage[ISO]{diffcoeff}
  - \usepackage{pxfonts}
---

Calvin KleinとKullback–Leibler divergenceがなんか似ていると常々思っている．



>* Calvin Klein ：　アメリカのファッションデザイナー，また同名の世界的ファッションブランド
>* Kullback–Leibler divergence : 確率論と情報理論における2つの確率分布の差異を計る尺度


語感が似ているだけで，まったく関係ないように見える．しかし私にとっては，愛用の香水ブランドがCalvin Kleinであったり，データ解析でKullback–Leibler divergenceを使ったりと両単語はなにかと目にすることが多い．




ちなみに，
離散確率分布$P,Q$に対して，$P$の$Q$に対するKL-divergenceは以下のように定義される：

$$
D_{\mathrm{KL}}
\right( 
P || Q
\left)
\equiv \sum_{i} P(i) \log \dfrac{P(i)}{Q(i)}
$$


また，連続確率分布$P,Q$に対して，$P$の$Q$に対するKL-divergenceは以下のように定義される：

$$
D_{\mathrm{KL}}
\right( 
P || Q
\left)
\equiv \int^{\infty}_{-\infty} P(x) \log \dfrac{p(i)}{q(i)}
\mathrm{d} t
$$


定義から明らかなように，divergenceと名乗っているが，距離の公理（非負性，非退化性，対称性，三角不等式）を満たさないので，数学的に厳密な距離ではない．